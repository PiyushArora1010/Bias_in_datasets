{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.attr_dataset import AttributeDataset_bffhq\n",
    "from module.loss import GeneralizedCELoss,EMA,MultiDimAverageMeter\n",
    "from module.models import dic_models\n",
    "from module.models2 import dic_models_2\n",
    "from data.util import get_dataset, IdxDataset, ZippedDataset, get_dataset_bffhq\n",
    "import torchvision.transforms.functional as TF\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "import random\n",
    "from numpy.random import RandomState\n",
    "\n",
    "def set_seed(seed: int) -> RandomState:\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # set to false for reproducibility, True to boost performance\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    random_state = random.getstate()\n",
    "    os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "    return random_state\n",
    "\n",
    "\n",
    "dataset_in = input(\"The dataset is: \")\n",
    "model_in = input(\"The model is: \")\n",
    "train_samples = int(input(\"The number of training samples is: \"))\n",
    "bias_ratio = float(input(\"The bias ratio is: \"))\n",
    "seed = int(input(\"The seed is: \"))\n",
    "\n",
    "set_seed(seed)\n",
    "\n",
    "target_attr_idx = 0\n",
    "bias_attr_idx = 1\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(filename, text):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(text)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(mw_model, test_loader):\n",
    "  mw_model.eval()\n",
    "  mw_correct = 0\n",
    "  with torch.no_grad():\n",
    "    for _, data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target[:,target_attr_idx]\n",
    "        target = target.to(device)\n",
    "\n",
    "        mw_outputs  = mw_model(data)\n",
    "        mw_pred = mw_outputs.data.max(1, keepdim=True)[1]\n",
    "\n",
    "        mw_correct += mw_pred.eq(target.data.view_as(mw_pred)).sum().item()\n",
    "  mw_accuracy = 100.*(torch.true_divide(mw_correct,len(test_loader.dataset))).item()\n",
    "  mw_model.train()\n",
    "  return mw_accuracy\n",
    "\n",
    "def evaluate_ssl(model, test_loader):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  with torch.no_grad():\n",
    "    for _, images, labels in test_loader:\n",
    "        labels = torch.zeros(len(labels))\n",
    "        images_90 = TF.rotate(images, 90)\n",
    "        labels_90 = torch.ones(len(labels))\n",
    "        images_180 = TF.rotate(images, 180)\n",
    "        labels_180 = torch.ones(len(labels))*2\n",
    "        images_270 = TF.rotate(images, 270)\n",
    "        labels_270 = torch.ones(len(labels))*3\n",
    "        images = torch.cat((images, images_90, images_180, images_270), dim=0)\n",
    "        labels = torch.cat((labels, labels_90, labels_180, labels_270), dim=0)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        del images_90, images_180, images_270, labels_90, labels_180, labels_270\n",
    "\n",
    "        outputs  = model(images)\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "\n",
    "        correct += pred.eq(labels.data.view_as(pred)).sum().item()\n",
    "  accuracy = 100.*(torch.true_divide(correct,len(test_loader.dataset)*4)).item()\n",
    "  model.train()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = get_dataset(\n",
    "        dataset_in,\n",
    "        data_dir='/home/user/datasets/debias',\n",
    "        dataset_split=\"train\",\n",
    "        transform_split=\"train\",\n",
    "    )\n",
    "test_dataset = get_dataset(\n",
    "        dataset_in,\n",
    "        data_dir='/home/user/datasets/debias',\n",
    "        dataset_split=\"eval\",\n",
    "        transform_split=\"eval\",\n",
    "    )\n",
    "valid_dataset = get_dataset(\n",
    "        dataset_in,\n",
    "        data_dir='/home/user/datasets/debias',\n",
    "        dataset_split=\"train\",\n",
    "        transform_split=\"train\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Size of the Dataset][2000]\n",
      "[Conflicting Samples in Training Data][100]\n",
      "[Conflicting Samples in Validation Data][1000]\n"
     ]
    }
   ],
   "source": [
    "indices_train_biased = train_dataset.attr[:,0] == train_dataset.attr[:,1]\n",
    "\n",
    "indices_train_biased = indices_train_biased.nonzero().squeeze()\n",
    "\n",
    "nums_train_biased = np.random.choice(indices_train_biased, int(train_samples - bias_ratio * train_samples) , replace=False)\n",
    "\n",
    "\n",
    "indices_train_unbiased = train_dataset.attr[:,0] != train_dataset.attr[:,1]\n",
    "\n",
    "indices_train_unbiased = indices_train_unbiased.nonzero().squeeze()\n",
    "\n",
    "nums_train_unbiased = np.random.choice(indices_train_unbiased, int(bias_ratio * train_samples) , replace=False)\n",
    "\n",
    "nums_train = np.concatenate((nums_train_biased, nums_train_unbiased))\n",
    "\n",
    "\n",
    "nums_valid_unbiased = []\n",
    "\n",
    "while len(nums_valid_unbiased) < 1000:\n",
    "    i = np.random.randint(0, len(valid_dataset))\n",
    "    if valid_dataset.attr[i,0] != valid_dataset.attr[i,1] and i not in nums_train:\n",
    "        nums_valid_unbiased.append(i)\n",
    "\n",
    "nums_valid_unbiased = np.array(nums_valid_unbiased)\n",
    "\n",
    "train_dataset.attr = train_dataset.attr[nums_train]\n",
    "train_dataset.data = train_dataset.data[nums_train]\n",
    "train_dataset.__len__ = train_samples\n",
    "train_dataset.query_attr = train_dataset.attr[:, torch.arange(2)]\n",
    "\n",
    "\n",
    "valid_dataset.attr = valid_dataset.attr[nums_valid_unbiased]\n",
    "valid_dataset.data = valid_dataset.data[nums_valid_unbiased]\n",
    "valid_dataset.__len__ = 1000\n",
    "valid_dataset.query_attr = valid_dataset.attr[:, torch.arange(2)]\n",
    "\n",
    "print(\"[Size of the Dataset][\"+str(len(train_dataset))+\"]\")\n",
    "print(\"[Conflicting Samples in Training Data][\"+str(len(train_dataset.attr[train_dataset.attr[:,0] != train_dataset.attr[:,1]]))+\"]\")\n",
    "print(\"[Conflicting Samples in Validation Data][\"+str(len(valid_dataset.attr[valid_dataset.attr[:,0] != valid_dataset.attr[:,1]]))+\"]\")\n",
    "\n",
    "train_target_attr = train_dataset.attr[:, target_attr_idx]\n",
    "train_bias_attr = train_dataset.attr[:, bias_attr_idx]\n",
    "\n",
    "del indices_train_biased, indices_train_unbiased, nums_train_biased, nums_train_unbiased, nums_train, nums_valid_unbiased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr_dims = []\n",
    "attr_dims.append(torch.max(train_target_attr).item() + 1)\n",
    "num_classes = attr_dims[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IdxDataset(train_dataset)\n",
    "valid_dataset = IdxDataset(valid_dataset)    \n",
    "test_dataset = IdxDataset(test_dataset)\n",
    " \n",
    "train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=250,\n",
    "        shuffle=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=250,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=250,\n",
    "        shuffle=False,\n",
    "        drop_last=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_d = dic_models[model_in](num_classes).to(device)\n",
    "    model_b = dic_models[model_in](num_classes).to(device)\n",
    "except:\n",
    "    model_d = dic_models_2[model_in](num_classes).to(device)\n",
    "    model_b = dic_models_2[model_in](num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'MNIST' in dataset_in:\n",
    "    optimizer_b = torch.optim.Adam(model_b.parameters(),lr= 0.002, weight_decay=0.0)\n",
    "    optimizer_d = torch.optim.Adam(model_d.parameters(),lr= 0.002, weight_decay=0.0)\n",
    "    schedulerd = MultiStepLR(optimizer_d, milestones=[300], gamma=0.5)\n",
    "    schedulerb = MultiStepLR(optimizer_b, milestones=[300], gamma=0.5)\n",
    "else:\n",
    "    optimizer_b = torch.optim.SGD(model_b.parameters(),lr= 0.1, weight_decay=5e-4, momentum = 0.9, nesterov = True)\n",
    "    optimizer_d = torch.optim.SGD(model_d.parameters(),lr= 0.1, weight_decay=5e-4, momentum = 0.9, nesterov = True)\n",
    "    schedulerd = MultiStepLR(optimizer_d, milestones=[150,225], gamma=0.1)\n",
    "    schedulerb = MultiStepLR(optimizer_b, milestones=[150,225], gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "bias_criterion = GeneralizedCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_loss_ema_b = EMA(torch.LongTensor(train_target_attr), alpha=0.7)\n",
    "sample_loss_ema_d = EMA(torch.LongTensor(train_target_attr), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'CIFAR' in dataset_in:\n",
    "    main_num_steps = 300\n",
    "elif 'MNIST' in dataset_in:\n",
    "    main_num_steps = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = -1.0\n",
    "test_cheat = -1.0\n",
    "test_accuracy_epoch = -1.0\n",
    "valid_accuracy_best = -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "SSL (Rotation)\n",
    "'''\n",
    "def rotation_ssl(model, train_loader,test_loader, epochs = 100):\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, 4).to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr= 0.001, weight_decay=0.0)\n",
    "    loss = -1.0\n",
    "    for epoch in range(epochs):\n",
    "     for idx, images, labels in train_loader:\n",
    "        '''\n",
    "        Preparing Data\n",
    "        '''\n",
    "        labels = torch.zeros(len(labels))\n",
    "        images_90 = TF.rotate(images, 90)\n",
    "        labels_90 = torch.ones(len(labels))\n",
    "        images_180 = TF.rotate(images, 180)\n",
    "        labels_180 = torch.ones(len(labels))*2\n",
    "        images_270 = TF.rotate(images, 270)\n",
    "        labels_270 = torch.ones(len(labels))*3\n",
    "        images = torch.cat((images, images_90, images_180, images_270), dim=0)\n",
    "        labels = torch.cat((labels, labels_90, labels_180, labels_270), dim=0)\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        del images_90, images_180, images_270, labels_90, labels_180, labels_270\n",
    "\n",
    "        logits = model(images)\n",
    "        loss = loss_func(logits, labels.long())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "     print(\"[Loss][\"+str(loss.item())+\"]\")\n",
    "     print(\"[Epoch][\"+str(epoch)+\"]\"+\"[\"+str(evaluate_ssl(model, test_loader))+\"]\")\n",
    "        \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss][1.3920453786849976]\n",
      "[Epoch][0][27.094998955726624]\n",
      "[Loss][1.4237620830535889]\n",
      "[Epoch][1][25.552499294281006]\n",
      "[Loss][1.3167458772659302]\n",
      "[Epoch][2][33.23250114917755]\n",
      "[Loss][1.27237868309021]\n",
      "[Epoch][3][36.2075001001358]\n",
      "[Loss][1.2242575883865356]\n",
      "[Epoch][4][44.17249858379364]\n",
      "[Loss][1.1233775615692139]\n",
      "[Epoch][5][49.367499351501465]\n",
      "[Loss][1.1103804111480713]\n",
      "[Epoch][6][49.767500162124634]\n",
      "[Loss][1.0631190538406372]\n",
      "[Epoch][7][51.69000029563904]\n",
      "[Loss][1.0722341537475586]\n",
      "[Epoch][8][52.14250087738037]\n",
      "[Loss][0.9850783348083496]\n",
      "[Epoch][9][51.81249976158142]\n",
      "[Loss][1.0957003831863403]\n",
      "[Epoch][10][52.42999792098999]\n",
      "[Loss][1.0427029132843018]\n",
      "[Epoch][11][52.38249897956848]\n",
      "[Loss][0.9479535818099976]\n",
      "[Epoch][12][52.9574990272522]\n",
      "[Loss][1.0135059356689453]\n",
      "[Epoch][13][53.72750163078308]\n",
      "[Loss][0.954334557056427]\n",
      "[Epoch][14][54.34499979019165]\n",
      "[Loss][1.036258578300476]\n",
      "[Epoch][15][55.059999227523804]\n",
      "[Loss][1.0250844955444336]\n",
      "[Epoch][16][55.537497997283936]\n",
      "[Loss][0.9372804760932922]\n",
      "[Epoch][17][55.56750297546387]\n",
      "[Loss][0.9281517863273621]\n",
      "[Epoch][18][54.85749840736389]\n",
      "[Loss][0.8642195463180542]\n",
      "[Epoch][19][56.587499380111694]\n",
      "[Loss][0.8680048584938049]\n",
      "[Epoch][20][56.16750121116638]\n",
      "[Loss][0.8604795336723328]\n",
      "[Epoch][21][57.260000705718994]\n",
      "[Loss][0.8919073343276978]\n",
      "[Epoch][22][56.11500144004822]\n",
      "[Loss][0.9082159399986267]\n",
      "[Epoch][23][56.53749704360962]\n",
      "[Loss][0.9289544820785522]\n",
      "[Epoch][24][55.972498655319214]\n",
      "[Loss][0.8635649681091309]\n",
      "[Epoch][25][57.60499835014343]\n",
      "[Loss][0.899900496006012]\n",
      "[Epoch][26][57.6324999332428]\n",
      "[Loss][0.8363726139068604]\n",
      "[Epoch][27][57.557499408721924]\n",
      "[Loss][0.8374693989753723]\n",
      "[Epoch][28][57.9800009727478]\n",
      "[Loss][0.8351175785064697]\n",
      "[Epoch][29][55.642497539520264]\n",
      "[Loss][0.8569681644439697]\n",
      "[Epoch][30][58.777499198913574]\n",
      "[Loss][0.8308334946632385]\n",
      "[Epoch][31][57.03499913215637]\n",
      "[Loss][0.8672974705696106]\n",
      "[Epoch][32][56.55500292778015]\n",
      "[Loss][0.8814352750778198]\n",
      "[Epoch][33][58.09999704360962]\n",
      "[Loss][0.8377546668052673]\n",
      "[Epoch][34][59.97750163078308]\n",
      "[Loss][0.8111781477928162]\n",
      "[Epoch][35][59.67249870300293]\n",
      "[Loss][0.8700387477874756]\n",
      "[Epoch][36][58.137500286102295]\n",
      "[Loss][0.7798638343811035]\n",
      "[Epoch][37][58.56500267982483]\n",
      "[Loss][0.7549176812171936]\n",
      "[Epoch][38][58.55000019073486]\n",
      "[Loss][0.7925317883491516]\n",
      "[Epoch][39][58.412498235702515]\n",
      "[Loss][0.731168270111084]\n",
      "[Epoch][40][57.520002126693726]\n",
      "[Loss][0.6984720230102539]\n",
      "[Epoch][41][59.50750112533569]\n",
      "[Loss][0.8163281679153442]\n",
      "[Epoch][42][58.432501554489136]\n",
      "[Loss][0.7141953110694885]\n",
      "[Epoch][43][59.394997358322144]\n",
      "[Loss][0.7300931215286255]\n",
      "[Epoch][44][58.789998292922974]\n",
      "[Loss][0.7544493675231934]\n",
      "[Epoch][45][59.60000157356262]\n",
      "[Loss][0.7057923078536987]\n",
      "[Epoch][46][59.815001487731934]\n",
      "[Loss][0.7200114130973816]\n",
      "[Epoch][47][59.32999849319458]\n",
      "[Loss][0.7003876566886902]\n",
      "[Epoch][48][59.960001707077026]\n",
      "[Loss][0.7443399429321289]\n",
      "[Epoch][49][59.654998779296875]\n",
      "[Loss][0.7554139494895935]\n",
      "[Epoch][50][58.934998512268066]\n",
      "[Loss][0.6894003748893738]\n",
      "[Epoch][51][56.50249719619751]\n",
      "[Loss][0.7298551797866821]\n",
      "[Epoch][52][56.7674994468689]\n",
      "[Loss][0.702257513999939]\n",
      "[Epoch][53][58.662497997283936]\n",
      "[Loss][0.7296648621559143]\n",
      "[Epoch][54][59.60249900817871]\n",
      "[Loss][0.6993688941001892]\n",
      "[Epoch][55][59.14750099182129]\n",
      "[Loss][0.6490890979766846]\n",
      "[Epoch][56][58.789998292922974]\n",
      "[Loss][0.6861127614974976]\n",
      "[Epoch][57][54.680001735687256]\n",
      "[Loss][0.6809768080711365]\n",
      "[Epoch][58][58.775001764297485]\n",
      "[Loss][0.7192368507385254]\n",
      "[Epoch][59][58.64750146865845]\n",
      "[Loss][0.6387248635292053]\n",
      "[Epoch][60][60.38249731063843]\n",
      "[Loss][0.5814593434333801]\n",
      "[Epoch][61][56.62500262260437]\n",
      "[Loss][0.6297587156295776]\n",
      "[Epoch][62][59.22999978065491]\n",
      "[Loss][0.6693322062492371]\n",
      "[Epoch][63][58.22499990463257]\n",
      "[Loss][0.6721848249435425]\n",
      "[Epoch][64][58.957499265670776]\n",
      "[Loss][0.622194230556488]\n",
      "[Epoch][65][60.10749936103821]\n",
      "[Loss][0.6253188252449036]\n",
      "[Epoch][66][59.12500023841858]\n",
      "[Loss][0.5473655462265015]\n",
      "[Epoch][67][57.842499017715454]\n",
      "[Loss][0.607890784740448]\n",
      "[Epoch][68][57.96999931335449]\n",
      "[Loss][0.6390177607536316]\n",
      "[Epoch][69][59.49000120162964]\n",
      "[Loss][0.5406259298324585]\n",
      "[Epoch][70][59.002500772476196]\n",
      "[Loss][0.636758029460907]\n",
      "[Epoch][71][57.01000094413757]\n",
      "[Loss][0.6004538536071777]\n",
      "[Epoch][72][58.68750214576721]\n",
      "[Loss][0.5297412872314453]\n",
      "[Epoch][73][58.53250026702881]\n",
      "[Loss][0.5461506247520447]\n",
      "[Epoch][74][56.812500953674316]\n",
      "[Loss][0.5545597076416016]\n",
      "[Epoch][75][58.364999294281006]\n",
      "[Loss][0.5962874889373779]\n",
      "[Epoch][76][58.71250033378601]\n",
      "[Loss][0.5922685265541077]\n",
      "[Epoch][77][59.780001640319824]\n",
      "[Loss][0.5550745129585266]\n",
      "[Epoch][78][59.23500061035156]\n",
      "[Loss][0.4660160541534424]\n",
      "[Epoch][79][59.12500023841858]\n",
      "[Loss][0.5014949440956116]\n",
      "[Epoch][80][58.2099974155426]\n",
      "[Loss][0.47881880402565]\n",
      "[Epoch][81][58.20000171661377]\n",
      "[Loss][0.6341254115104675]\n",
      "[Epoch][82][55.73999881744385]\n",
      "[Loss][0.6077250242233276]\n",
      "[Epoch][83][58.492499589920044]\n",
      "[Loss][0.5570911765098572]\n",
      "[Epoch][84][58.230000734329224]\n",
      "[Loss][0.5336625576019287]\n",
      "[Epoch][85][59.277498722076416]\n",
      "[Loss][0.5050719380378723]\n",
      "[Epoch][86][59.062498807907104]\n",
      "[Loss][0.5270363688468933]\n",
      "[Epoch][87][59.82000231742859]\n",
      "[Loss][0.47145771980285645]\n",
      "[Epoch][88][59.757500886917114]\n",
      "[Loss][0.46552109718322754]\n",
      "[Epoch][89][58.55749845504761]\n",
      "[Loss][0.45908668637275696]\n",
      "[Epoch][90][58.19249749183655]\n",
      "[Loss][0.4642369747161865]\n",
      "[Epoch][91][58.7024986743927]\n",
      "[Loss][0.45240065455436707]\n",
      "[Epoch][92][59.97999906539917]\n",
      "[Loss][0.48463553190231323]\n",
      "[Epoch][93][60.13000011444092]\n",
      "[Loss][0.4354843497276306]\n",
      "[Epoch][94][56.52499794960022]\n",
      "[Loss][0.47724902629852295]\n",
      "[Epoch][95][58.51749777793884]\n",
      "[Loss][0.4638254642486572]\n",
      "[Epoch][96][57.99000263214111]\n",
      "[Loss][0.45500487089157104]\n",
      "[Epoch][97][57.91249871253967]\n",
      "[Loss][0.4138556122779846]\n",
      "[Epoch][98][59.584999084472656]\n",
      "[Loss][0.41836798191070557]\n",
      "[Epoch][99][59.21499729156494]\n"
     ]
    }
   ],
   "source": [
    "model_temp = rotation_ssl(model_d, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_d = model_temp\n",
    "model_d.fc = nn.Linear(model_d.fc.in_features, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1][Train Accuracy 23.8 ][Validation Accuracy 18.0 ]\n",
      "[Test Accuracy cheat][18.4600]\n",
      "[Best Test Accuracy] 18.459999561309814\n",
      "[Epoch 2][Train Accuracy 31.85 ][Validation Accuracy 22.2 ]\n",
      "[Test Accuracy cheat][22.1600]\n",
      "[Best Test Accuracy] 22.15999960899353\n",
      "[Epoch 3][Train Accuracy 37.2 ][Validation Accuracy 24.4 ]\n",
      "[Test Accuracy cheat][24.8200]\n",
      "[Best Test Accuracy] 24.819999933242798\n",
      "[Epoch 4][Train Accuracy 44.7 ][Validation Accuracy 24.6 ]\n",
      "[Test Accuracy cheat][24.8800]\n",
      "[Best Test Accuracy] 24.879999458789825\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ASUS\\Desktop\\LfFMW\\jupyter\\Train_LfF_SSL.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/LfFMW/jupyter/Train_LfF_SSL.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, main_num_steps\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/LfFMW/jupyter/Train_LfF_SSL.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mfor\u001b[39;00m ix, (index,data,attr) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/LfFMW/jupyter/Train_LfF_SSL.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ASUS/Desktop/LfFMW/jupyter/Train_LfF_SSL.ipynb#X21sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         attr \u001b[39m=\u001b[39m attr\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\machineLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\machineLearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\machineLearning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\machineLearning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\LfFMW\\data\\util.py:26\u001b[0m, in \u001b[0;36mIdxDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[1;32m---> 26\u001b[0m     \u001b[39mreturn\u001b[39;00m (idx, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx])\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\Desktop\\LfFMW\\data\\attr_dataset.py:38\u001b[0m, in \u001b[0;36mAttributeDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     36\u001b[0m image, attr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[index], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_attr[index]\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 38\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(image)\n\u001b[0;32m     40\u001b[0m \u001b[39mreturn\u001b[39;00m image, attr\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\machineLearning\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\machineLearning\\lib\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32mc:\\Users\\ASUS\\anaconda3\\envs\\machineLearning\\lib\\site-packages\\torchvision\\transforms\\functional.py:155\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    153\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    154\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mdefault_float_dtype)\u001b[39m.\u001b[39;49mdiv(\u001b[39m255\u001b[39;49m)\n\u001b[0;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    157\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for step in range(1, main_num_steps+1):\n",
    "\n",
    "    for ix, (index,data,attr) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        attr = attr.to(device)\n",
    "\n",
    "        label = attr[:, target_attr_idx]\n",
    "        bias_label = attr[:, bias_attr_idx]\n",
    "\n",
    "        logit_b = model_b(data)\n",
    "        logit_d = model_d(data)\n",
    "\n",
    "\n",
    "        loss_b = criterion(logit_b, label).cpu().detach()\n",
    "        loss_d = criterion(logit_d, label).cpu().detach()\n",
    "        \n",
    "        loss_per_sample_b = loss_b\n",
    "        loss_per_sample_d = loss_d\n",
    "        \n",
    "        # EMA sample loss\n",
    "        sample_loss_ema_b.update(loss_b, index)\n",
    "        sample_loss_ema_d.update(loss_d, index)\n",
    "        \n",
    "        # class-wise normalize\n",
    "        loss_b = sample_loss_ema_b.parameter[index].clone().detach()\n",
    "        loss_d = sample_loss_ema_d.parameter[index].clone().detach()\n",
    "        \n",
    "        label_cpu = label.cpu()\n",
    "        \n",
    "        for c in range(num_classes):\n",
    "            class_index = np.where(label_cpu == c)[0]\n",
    "            max_loss_b = sample_loss_ema_b.max_loss(c)\n",
    "            max_loss_d = sample_loss_ema_d.max_loss(c)\n",
    "            loss_b[class_index] /= max_loss_b\n",
    "            loss_d[class_index] /= max_loss_d\n",
    "   \n",
    "        # re-weighting based on loss value / generalized CE for biased model\n",
    "        loss_weight = loss_b / (loss_b + loss_d + 1e-8)\n",
    "\n",
    "        loss_b_update = bias_criterion(logit_b, label)\n",
    "\n",
    "        loss_d_update = criterion(logit_d, label) * loss_weight.to(device)\n",
    "\n",
    "        loss = loss_b_update.mean() + loss_d_update.mean()\n",
    "\n",
    "        optimizer_b.zero_grad()\n",
    "        optimizer_d.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_b.step()\n",
    "        optimizer_d.step()\n",
    "    \n",
    "    schedulerb.step()\n",
    "    schedulerd.step()\n",
    "\n",
    "    train_accuracy_epoch = evaluate_accuracy(model_d, train_loader)\n",
    "    prev_valid_accuracy = valid_accuracy_best\n",
    "    valid_accuracy_epoch = evaluate_accuracy(model_d, valid_loader)\n",
    "    valid_accuracy_best = max(valid_accuracy_best, valid_accuracy_epoch)\n",
    "\n",
    "    print(\"[Epoch \"+str(step)+\"][Train Accuracy\", round(train_accuracy_epoch,4),\"][Validation Accuracy\",round(valid_accuracy_epoch,4),\"]\")\n",
    "\n",
    "    test_accuracy_epoch = evaluate_accuracy(model_d, test_loader)\n",
    "\n",
    "    test_cheat = max(test_cheat, test_accuracy_epoch)\n",
    "\n",
    "    print(\"[Test Accuracy cheat][%.4f]\"%test_cheat)\n",
    "\n",
    "    if valid_accuracy_best > prev_valid_accuracy:\n",
    "        test_accuracy = test_accuracy_epoch\n",
    "\n",
    "    print('[Best Test Accuracy]', test_accuracy)\n",
    "\n",
    "\n",
    "write_to_file('results_text/results_SSL_LfF_'+dataset_in.split('-')[0]+'_'+str(train_samples)+'_'+str(bias_ratio)+'.txt','[Best Test Accuracy]'+str(test_accuracy)+\"[Final Epoch Test Accuracy]\"+str(test_accuracy_epoch)+ '[Best Cheat Test Accuracy]'+str(test_cheat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('interspeech')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1485d274e04ef4a7138328b30d897827ea74add48c3ae6df1098ff4d330f7ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
